# @package _global_
training:
  epochs: 10
  batch_size: 128
  learning_rate: 0.001
  optimizer: adam
  criterion: cross_entropy
  
  # Gradient accumulation
  gradient_accumulation_steps: 1
  
  # Checkpointing
  save_dir: ./finetune
  save_best: true
  save_last: true
  
  # Logging
  log_interval: 100  # Log every N batches
  
  # Early stopping
  early_stopping:
    enabled: false
    patience: 5
    min_delta: 0.001




